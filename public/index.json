[{"authors":null,"categories":null,"content":"I am a Postdoctoral Fellow at the Baker Lab which is part of the University of Washington Institute for Protein Design. My research interests lie in the application of mathematics and physics to complex and biological systems. I am particularly interested protein structure prediction. ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"https://dauparas.github.io/author/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/admin/","section":"author","summary":"I am a Postdoctoral Fellow at the Baker Lab which is part of the University of Washington Institute for Protein Design. My research interests lie in the application of mathematics and physics to complex and biological systems. I am particularly interested protein structure prediction. ","tags":null,"title":"Justas Dauparas","type":"author"},{"authors":[],"categories":[],"content":"\nBayesian inference\nModelling sequences\nWe would like to model a distribution of some sequences of lenght $L$ where every element in the sequence can take $K$ different values. Write the parametric probability distribution for the sequence as a joint probability of elements in the sequence, i.e. \\begin{align} p(\\mathbf{x};\\boldsymbol{\\theta})=p(x_1, x_2,\u0026hellip;, x_L;\\boldsymbol{\\theta}), \\end{align} where $\\boldsymbol{\\theta}$ are parameters of the distribution. Now suppose that we got $N$ examples drawn from this distribution. Denote the data as $\\mathbf{X}\\in \\mathbb{R}^{N\\times L\\times K}$. The goal is to find $\\boldsymbol{\\theta}$ that explains the data.\n\\begin{align} p(\\boldsymbol{\\theta}\\vert \\mathbf{X}) = \\frac{p(\\mathbf{X}\\vert \\boldsymbol{\\theta})p(\\boldsymbol{\\theta})}{\\sum_{\\boldsymbol{\\theta}}p(\\mathbf{X}\\vert \\boldsymbol{\\theta})p(\\boldsymbol{\\theta})}. \\end{align}\nPartition function\nSuppose we come up with a probabilistic model which has an unnormalized probability distribution $\\tilde{p}(\\mathbf{x} ; \\boldsymbol{\\theta})$. To obtain a normalized probability distribution $p(\\mathbf{x} ; \\boldsymbol{\\theta})$ we divide by the sum of all possible values that $\\mathbf{x}$ can take, i.e. $$p(\\mathbf{x} ; \\boldsymbol{\\theta})=\\frac{\\tilde{p}(\\mathbf{x} ; \\boldsymbol{\\theta})}{\\sum_{\\mathbf{x}}\\tilde{p}(\\mathbf{x} ; \\boldsymbol{\\theta})}=\\frac{\\tilde{p}(\\mathbf{x} ; \\boldsymbol{\\theta})}{Z(\\boldsymbol{\\theta})}.$$ The letter Z stands for the German word Zustandssumme, \u0026ldquo;sum over states\u0026rdquo;. It is called the partition function because it encodes how the probabilities are partitioned among the different states, based on their individual unnormalized probabilities. The problem is that the partition function often cannot be calculated exactly due to the large number of elements in the sum. For sequences we would have $K^L$ elements. The gradient of the log-likelihood with respect to the parameters has a term corresponding to the gradient of the partition function: $$\\nabla_{\\boldsymbol{\\theta}}\\log{p(\\mathbf{x} ; \\boldsymbol{\\theta})}= \\nabla_{\\boldsymbol{\\theta}}\\log{\\tilde{p}(\\mathbf{x} ; \\boldsymbol{\\theta})-\\nabla_{\\boldsymbol{\\theta}}\\log{Z(\\boldsymbol{\\theta})}}$$ and this makes learning the model often intractable. For models that have non-zero probability for every state, i.e. $\\tilde{p}(\\mathbf{x} ; \\boldsymbol{\\theta})\u0026gt;0$ for all $\\mathbf{x}$, we can write $\\tilde{p}(\\mathbf{x} ; \\boldsymbol{\\theta})=\\exp (\\log \\tilde{p}(\\mathbf{x}))$ and then $$\\begin{align} \\label{eq:1} \\nabla_{\\boldsymbol{\\theta}} \\log Z(\\boldsymbol{\\theta})\u0026amp;=\\frac{\\sum_{\\mathbf{x}} \\nabla_{\\boldsymbol{\\theta}} \\tilde{p}(\\mathbf{x}; \\boldsymbol{\\theta})}{Z(\\boldsymbol{\\theta})}=\\frac{\\sum_{\\mathbf{x}} \\nabla_{\\boldsymbol{\\theta}} \\exp (\\log \\tilde{p}(\\mathbf{x}))}{Z}=\\sum_{\\mathbf{x}} p(\\mathbf{x}) \\nabla_{\\boldsymbol{\\theta}} \\log \\tilde{p}(\\mathbf{x}), \\\\\n\\color{green}{\\nabla_{\\boldsymbol{\\theta}} \\log Z(\\boldsymbol{\\theta})}\u0026amp;\\color{green}{=}\\color{green}{\\mathbb{E}_{\\mathbf{x} \\sim p(\\mathbf{x})} \\nabla_{\\boldsymbol{\\theta}} \\log \\tilde{p}(\\mathbf{x})}. \\end{align}$$ The last identity is the basis (hence in green) for a variety of Monte Carlo methods for approximately maximizing the likelihood of models with intractable partition functions. Read more about partition function here: https://www.deeplearningbook.org/contents/partition.html .\nPseudolikelihood\nNotice that it is easy to compute ratios of probabilities in unnormalized probabilistic models. This is because the partition function appears in both the numerator and the denominator of the ratio and cancels out: \\begin{align} \\frac{p(\\mathbf{x};\\boldsymbol{\\theta})}{p(\\mathbf{y};\\boldsymbol{\\theta})}=\\frac{\\frac{1}{Z} \\tilde{p}(\\mathbf{x};\\boldsymbol{\\theta})}{\\frac{1}{Z} \\tilde{p}(\\mathbf{y};\\boldsymbol{\\theta})}=\\frac{\\tilde{p}(\\mathbf{x};\\boldsymbol{\\theta})}{\\tilde{p}(\\mathbf{y};\\boldsymbol{\\theta})}. \\end{align}\nMoreover, using the chain rule of probability we can write the log-likelihood: \\begin{align} \\log p(\\mathbf{x};\\boldsymbol{\\theta})=\\log p\\left(x_{1};\\boldsymbol{\\theta}\\right)+\\log p\\left(x_{2} | x_{1};\\boldsymbol{\\theta}\\right)+\\cdots+p\\left(x_{L} | \\mathbf{x}_{1 : L-1};\\boldsymbol{\\theta}\\right). \\end{align} The pseudolikelihood (Besag, 1975) objective function is given by: \\begin{align} \\mathcal{L}_{pl}(\\mathbf{x};\\boldsymbol{\\theta})=\\sum_{\\ell=1}^{L} \\log p\\left(x_{\\ell} | \\boldsymbol{x}_{-\\ell};\\boldsymbol{\\theta}\\right) \\end{align} If each random variable has $K$ diﬀerent values, this requires only $K\\times L$ evaluations of $\\tilde{p}$ to compute, as opposed to the $K^L$ evaluations needed to compute the partition function.\nMarkov Random Field with pseudolikelihood\nFor the Markov Random Field we assume that given parameters $\\color{green}{\\mathbf{b}}\\in \\mathbb{R}^{L\\times K}$ and $\\color{red}{\\mathbf{W}}\\in \\mathbb{R}^{L\\times K\\times L\\times K}$ the positions are independent \\begin{align} \\tilde{p}(\\mathbf{x}\\vert\\boldsymbol{\\theta}) \u0026amp;= \\tilde{p}(\\mathbf{x}\\vert\\color{green}{\\mathbf{b}}, \\color{red}{\\mathbf{W}})=\\prod_{\\ell=1}^{L}\\tilde{p}(x_{\\ell}\\vert\\color{green}{\\mathbf{b}}, \\color{red}{\\mathbf{W}}),\\\\\n\\tilde{p}(x_{\\ell}\\vert\\color{green}{\\mathbf{b}}, \\color{red}{\\mathbf{W}})\u0026amp;=\\exp{\\left(\\sum_{k=1}^{K}\\color{green}{b_{\\ell k}}x_{\\ell k}+\\sum_{k=1}^{K}\\sum_{s=\\ell+1}^{L}\\sum_{r=1}^{K} x_{rs} \\color{red}{W_{rs\\ell k}}x_{\\ell k}\\right)} \\end{align}\nSuppose we are given $N$ sequences of length $L$ and every element in the sequence has an alphabet of size $K$. Denote this data by $\\mathbf{X}\\in \\mathbb{R}^{N\\times L\\times K}$. The sum over alphabet adds up to one for every sequence and every element, i.e. $\\sum_{k=1}^K X_{nlk} = 1$ for all $n\\in {1,2,\u0026hellip;,N}$ and $l\\in {1,2,\u0026hellip;,L}$. Take the simple model with two parameters \\begin{align} l(\\mathbf{x};\\color{green}{\\boldsymbol{b}},\\color{red}{\\boldsymbol{W}})=\\sum_{\\ell=1}^{L} \\log p\\left(x_{l} | \\boldsymbol{x}_{-l};\\color{green}{\\boldsymbol{b}},\\color{red}{\\boldsymbol{W}}\\right)=\\sum_{l=1}^{L} \\log{\\frac{p\\left(x_{l},\\boldsymbol{x}_{-l};\\color{green}{\\boldsymbol{b}},\\color{red}{\\boldsymbol{W}}\\right)}{\\sum_{x_l}p\\left(x_{l},\\boldsymbol{x}_{-l};\\color{green}{\\boldsymbol{b}},\\color{red}{\\boldsymbol{W}}\\right)}} \\end{align} \\begin{align} \\tilde{p}(\\mathbf{X}; \\color{green}{\\boldsymbol{b}},\\color{red}{\\boldsymbol{W}})\u0026amp;=\\prod_{l=1}^{L}\\exp{\\left(\\sum_{k=1}^{K}\\color{green}{b_{lk}}X_{lk}+\\sum_{k=1}^{K}\\sum_{s=l+1}^{L}\\sum_{r=1}^{K} X_{rs} \\color{red}{W_{rslk}}X_{lk}\\right)},\\\\\nZ(\\color{green}{\\boldsymbol{b}},\\color{red}{\\boldsymbol{W}})\u0026amp;=\\sum_{\\mathbf{X}}\\tilde{p}(\\mathbf{X}; \\color{green}{\\boldsymbol{b}},\\color{red}{\\boldsymbol{W}})=\\sum_{k_1=1}^{K}\\sum_{k_2=1}^{K}\u0026hellip;\\sum_{k_L=1}^{K}\\tilde{p}(\\mathbf{x_1}(k_1), \\mathbf{x_2}(k_2),\u0026hellip;\\mathbf{x_L}(k_L); \\color{green}{\\boldsymbol{b}},\\color{red}{\\boldsymbol{W}}),\\\\\nZ(\\color{green}{\\boldsymbol{b}},\\color{red}{\\boldsymbol{W}})\u0026amp;=\\prod_{l=1}^{L} \\sum \\end{align}\nIn progress!\n  Figure 1. The data matrix $\\mathbf{X}\\in \\mathbb{R}^{N\\times LK} = \\mathbb{R}^{N\\times L\\times K}$ has $N$ sequences, every sequence is of length $L$ and every position in the sequence can have $K$ different states. In this picture states are encoded using a one-hot representation.   ","date":1559904286,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559904286,"objectID":"b98b0d05337d0b7f16728ee3680cde51","permalink":"https://dauparas.github.io/post/multivariate/","publishdate":"2019-06-07T06:44:46-04:00","relpermalink":"/post/multivariate/","section":"post","summary":"Bayesian inference\nModelling sequences\nWe would like to model a distribution of some sequences of lenght $L$ where every element in the sequence can take $K$ different values. Write the parametric probability distribution for the sequence as a joint probability of elements in the sequence, i.e. \\begin{align} p(\\mathbf{x};\\boldsymbol{\\theta})=p(x_1, x_2,\u0026hellip;, x_L;\\boldsymbol{\\theta}), \\end{align} where $\\boldsymbol{\\theta}$ are parameters of the distribution. Now suppose that we got $N$ examples drawn from this distribution. Denote the data as $\\mathbf{X}\\in \\mathbb{R}^{N\\times L\\times K}$.","tags":[],"title":"Pseudolikelihood","type":"post"},{"authors":null,"categories":[],"content":"This is a link to theoretical physics.\n","date":1553873180,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553873180,"objectID":"ec8dcaf41bb54c4d82ded5037bc525c7","permalink":"https://dauparas.github.io/links/physics/","publishdate":"2019-03-29T11:26:20-04:00","relpermalink":"/links/physics/","section":"links","summary":"This is a link to theoretical physics.","tags":[],"title":"Theoretical Physics","type":"links"},{"authors":["Justas Dauparas","Haobo Wang","Peter Koo","Mor Nitzan","Sergey Ovchinnikov"],"categories":null,"content":"","date":1551661279,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551661279,"objectID":"6f3f622cfd3e848ed35db71e052c9b46","permalink":"https://dauparas.github.io/publication/unified/","publishdate":"2019-03-03T20:01:19-05:00","relpermalink":"/publication/unified/","section":"publication","summary":"Submitted to the ICML Workshop on Computational Biology.","tags":[],"title":"Unified framework for multivariate distributions in biological sequences","type":"publication"},{"authors":["Justas Dauparas","Katja Hofmann","Ryota Tomioka"],"categories":null,"content":"","date":1527639497,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527639497,"objectID":"6e6cd07e02f75752405666188a577387","permalink":"https://dauparas.github.io/publication/rl/","publishdate":"2018-05-29T19:18:17-05:00","relpermalink":"/publication/rl/","section":"publication","summary":"We show that Q-learning with nonlinear Q-function and no explicit exploration can learn several standard benchmark tasks.","tags":[],"title":"Depth and nonlinearity induce implicit exploration","type":"publication"},{"authors":["Xu Haoran","Justas Dauparas","Debasish Das","Eric Lauga","Yilin Wu"],"categories":null,"content":"","date":1519521906,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519521906,"objectID":"f8c0251014175e98e3f949ddc3e3938d","permalink":"https://dauparas.github.io/publication/self_organisation/","publishdate":"2018-02-24T20:25:06-05:00","relpermalink":"/publication/self_organisation/","section":"publication","summary":"We discovered that motile cells in sessile colonies of peritrichously flagellated bacteria can self-organize into motile bands that can drive long-range fluid transport at a constant speed of ~30 μm/s, providing a stable high-speed avenue for material transport at the colony scale.","tags":[],"title":"Self-organization of swimmers drives long-range fluid transport in bacterial colonies","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"62e3e13b516536879ad3e2adb5ab40ef","permalink":"https://dauparas.github.io/publication/corner_flows/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/corner_flows/","section":"publication","summary":"","tags":null,"title":"","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"cf3bfd228ce70b343a4995f2efd4d5de","permalink":"https://dauparas.github.io/publication/flagellar_flows/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/flagellar_flows/","section":"publication","summary":"","tags":null,"title":"","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"24892bd37ce9f61c7521bbb535f7c432","permalink":"https://dauparas.github.io/publication/helical_micropumps/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/helical_micropumps/","section":"publication","summary":"","tags":null,"title":"","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0f40cf545adf79432be77d9037b7794b","permalink":"https://dauparas.github.io/publication/thesis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/thesis/","section":"publication","summary":"","tags":null,"title":"","type":"publication"},{"authors":null,"categories":[],"content":"This is a link to theoretical physics.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f203a35d55569061480d498c648abd9a","permalink":"https://dauparas.github.io/talk/physics/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/talk/physics/","section":"talk","summary":"This is a link to theoretical physics.","tags":[],"title":"Theoretical Physics","type":"talk"}]